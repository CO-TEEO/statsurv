#' @title Extract fitted values from a model fit object
#'
#' @description A consistent method for extracting fitted values from a model fit object.
#'   `extract_yhat` returns point estimates for the fitted values while `sample_yhat` returns 1 or
#'   more random samples from the fitted values, using the variance-covariance matrix of the
#'   coefficients.
#'
#' @param space_coord A gridcoord object (\code{\link[gridcoord]{gc_gridcoord}}) describing the
#'   spatial area that is covered by `fit` and `data`.
#' @param time_coord A gridcoord object (\code{\link[gridcoord]{gc_gridcoord}}) describing the time
#'   span that is covered by `fit` and `data`, as well as any additional time points for which
#'   outcome variables should be predicted.  This coordinate must be ordered, with the first entries
#'   in the dataframe corresponding to the earliest time periods and the last entries corresponding
#'   to the most recent.
#' @param fit A model fit object, such as that generated by `lm` or `glm`.
#' @param data A data.frame containig the data that should be used to predict the fitted values.
#'   Must have all of the covariates used by the model fit object to calculate fitted values, as
#'   well as columns corresponding to `space_coord` and `time_coord`
#' @param ... Additional arguments passed into other methods
#'
#' @details For `lm`  and `glm` models, `extract_yhat` is a friendly wrapper around the
#'   \code{\link[stats]{predict}} method, but it can also extract fitted values from some models
#'   that do not have `predict` methods, including `INLA` and `forecast_ARIMA` models. `sample_yhat`
#'   uses the \code{\link[arm]{sim}} function to draw samples of the fitted values for `lm` and
#'   `glm` models.

#' @return A data.frame whose first two columns correspond to `space_coord` and `time_coord`, and
#'   whose remaining columns are the point estimates or random samples of the fitted values.
#' @describeIn extract_yhat Return point estimates for fitted values
#' @export
#' @seealso \code{\link{extract_yhat.forecast_ARIMA}}
#' @md
#' @examples
#' library("scanstatistics")
#' nm_county_coord <- statsurv::nm_county_coord
#' data(NM_popcas)
#' year_coord <- generate_date_range(lubridate::ymd("1973-01-01"),
#'                                   lubridate::ymd("1991-01-01"),
#'                                   time_division = "year")
#' year_coord$year <- year_coord$date_label
#' year_coord <- gridcoord::gc_gridcoord(year_coord, "year")
#'
#'
#'
#' # Fit a model to all the data, and then extract predictions:
#' fit <- glm(count ~ year,
#'            family = poisson(link = "log"),
#'            offset = log(population),
#'            data = NM_popcas)
#'
#' # Then use extract_yhat to get out predictions for our observed variable:
#' extract_yhat(nm_county_coord, year_coord, fit, NM_popcas)
#'
#' # Use sample_yhat to draw random samples from the output:
#' sample_yhat(nm_county_coord, year_coord, fit, NM_popcas, n_samples = 10)
#'
#' \dontrun{
#' # We can use INLA models if available as well:
#' fit_inla <- INLA::inla(count ~ year + f(county, model = "iid"),
#'                        family = "poisson",
#'                        control.family = list(link = "log"),
#'                        offset = log(population),
#'                        # These 2 options required to use extract_yhat and sample_yhat
#'                        control.predictor = list(compute = TRUE),
#'                        control.compute = list(config = TRUE),
#'                        data = NM_popcas)
#'
#' extract_yhat(nm_county_coord, year_coord, fit_inla, NM_popcas)
#'
#' sample_yhat(nm_county_coord, year_coord, fit_inla, NM_popcas, n_samples = 10)
#' }
extract_yhat <- function(space_coord, time_coord, fit, data, ...) {
 UseMethod("extract_yhat", fit)
}


#' @param n_samples The number of random samples of the fitted values to draw.
#' @describeIn extract_yhat Return random samples for fitted values
#' @export
sample_yhat <- function(space_coord, time_coord, fit, data, n_samples = 10) {
  UseMethod("sample_yhat", fit)
}


#' @export
extract_yhat.lm <- function(space_coord, time_coord, fit, data, ...) {
  sc_name <- gridcoord::gc_get_name(space_coord)
  tc_name <- gridcoord::gc_get_name(time_coord)
  yhat <- data.frame(v1 = data[[sc_name]],
                     v2 = data[[tc_name]],
                     v3 = stats::predict(fit, data, type = "response"),
                     stringsAsFactors = FALSE)
  colnames(yhat) <- c(sc_name, tc_name, "yhat")
  return(yhat)
}

#' @export
sample_yhat.lm <- function(space_coord, time_coord, fit, data, n_samples) {

  if (identical(class(fit), c("negbin", "glm", "lm"))) {
    # A workaround to get arm::sim to work on negbin models
    fit_faux <- fit
    class(fit_faux) <- c("glm", "lm")
    sim_coeffs <- arm::sim(fit_faux, n_samples)@coef
  } else {
    sim_coeffs <- arm::sim(fit, n_samples)@coef
  }
  sim_data <- apply(sim_coeffs, 1, predict_from_coeff, fit = fit, data = data)
  sim_data <- as.data.frame(sim_data)
  colnames(sim_data) <- paste0("sample", seq_len(n_samples))

  sc_name <- gridcoord::gc_get_name(space_coord)
  tc_name <- gridcoord::gc_get_name(time_coord)
  all_data <- cbind(data[[sc_name]],
                    data[[tc_name]],
                    sim_data,
                    stringsAsFactors = FALSE)
  colnames(all_data) <- c(sc_name, tc_name, colnames(sim_data))
  return(all_data)
}


#' @export
extract_yhat.inla <- function(space_coord, time_coord, fit, data, ...) {
  if (!requireNamespace("INLA", quietly = TRUE)) {
    stop("The 'INLA' package is required to use inla models in statsurv; ",
         "please install it before continuing")
  }
  # Depending on how INLA is set up, we can get lots of different values back
  # I'm always going to be using the linear predictor.
  sc_name <- gridcoord::gc_get_name(space_coord)
  tc_name <- gridcoord::gc_get_name(time_coord)

  inla_link <- get_inla_link(fit)


  if (is.null(fit$marginals.linear.predictor)) {
    stop("Unable to extract fitted values from this inla-object. ",
            "The inla-object must be computed with option ",
            "'control.predictor=list(compute = TRUE) to extract fitted values.")
  }

  linear_predictor_marginals <- fit$marginals.linear.predictor
  transform_to_fitted <- function(marginal) {
    INLA::inla.emarginal(inla_link, marginal, inverse = TRUE)
  }
  fitted_values <- vapply(linear_predictor_marginals, transform_to_fitted, numeric(1))

  # Fitted values does not include the exposure, so we need to account for that
  if (!is.null(fit$.args$E)) {
    fitted_values <- fitted_values * fit$.args$E
  }
  yhat <- data.frame(v1 = data[[sc_name]],
                     v2 = data[[tc_name]],
                     v3 = fitted_values,
                     stringsAsFactors = FALSE)
  colnames(yhat) <- c(sc_name, tc_name, "yhat")
  return(yhat)

}

#' @export
sample_yhat.inla <- function(space_coord, time_coord, fit, data, n_samples = 10) {
  smpl <- INLA::inla.posterior.sample(n = n_samples,
                                      result = fit,
                                      selection = list("Predictor" = 0))
  samples <- list_transpose(smpl)
  latent <- do.call(cbind, samples$latent)
  colnames(latent) <- paste0("sample", seq_len(ncol(latent)))


  inla_link <- get_inla_link(fit)
  fitted_values <- apply(latent, 2, inla_link, inverse = TRUE)

  # Incorporate the exposure
  if (!is.null(fit$.args$E)) {
    sim_data <- apply(fitted_values, 2, function(x) x * fit$.args$E)
  } else {
    sim_data <- fitted_values
  }
  sim_data <- as.data.frame(sim_data)
  rownames(sim_data) <- NULL

  sc_name <- gridcoord::gc_get_name(space_coord)
  tc_name <- gridcoord::gc_get_name(time_coord)
  all_data <- cbind(data[[sc_name]],
                    data[[tc_name]],
                    sim_data,
                    stringsAsFactors = FALSE)
  colnames(all_data) <- c(sc_name, tc_name, colnames(sim_data))

  if (any(is.na(fit$summary.fitted.values$mode))) {
    warning("<NA> values present in summary.fitted.values. ",
            "Extracted or sampled values may not be accurate")
  }
  if (is.null(fit$summary.fitted.values$mode)) {
    warning("Unable to compare sampled values with extracted values. ",
            "Impossible to know if sampled values are accurate. ",
            "Use  option 'control.predictor=list(compute = TRUE) to avoid this warning.")
  }
  return(all_data)
}


#' @export
extract_yhat.Arima <- function(space_coord, time_coord, fit, data, ...) {
  if (!requireNamespace("forecast", quietly = TRUE)) {
    stop("The 'forecast' package is required to use ARIMA models in statsurv; ",
         "please install it before continuing. ",
         "After installation, fit your models by calling 'forecast::Arima' rather than ",
         "arima")
  } else {
    stop("statsurv does not work with basic arima models. Please use 'forecast::Arima' instead of ",
         " the 'arima' function.")
  }
}

#' @title Extract fitted values from a forecast_ARIMA model object
#'
#'
#' @param xreg A matrix of covariate information used to calculate ARIMA values. Must cover the
#'   exact same temporal and spatial locations as `data`
#' @param n_ahead How many data points should be predicted?
#' @param ... Other arguments
#' @inheritParams extract_yhat
#'
#' @details ARIMA models currently only have limited support in the statsurv package. The syntax for
#'   fitting and working with ARIMA models is significantly different than that of `lm` or `glm`
#'   models. Some of the key differences are:
#' \enumerate{
#'   \item extract_yhat.forecast_ARIMA only works on models containing a single time series. In
#'   other words, the space coordinate must only have a single location.
#'   \item forecast_ARIMA models do not use the `data` parameter, instead any covariates must be
#'   included in the matrix `xreg`
#'   \item extract_yhat.forecast_ARIMA needs to know how many time-steps ahead should be predicted,
#'   as specified by the `n_ahead` parameter.
#'   }
#' @inherit extract_yhat return
#' @seealso \code{\link{extract_yhat}}, \code{\link[forecast]{forecast}}
#' @export
#' @md
#' @examples
#' \dontrun{
#' library("scanstatistics")
#' library("forecast")
#' nm_county_coord <- statsurv::nm_county_coord
#' data(NM_popcas)
#' year_coord <- generate_date_range(lubridate::ymd("1973-01-01"),
#'                                   lubridate::ymd("1991-01-01"),
#'                                   time_division = "year")
#' year_coord$year <- year_coord$date_label
#' year_coord <- gridcoord::gc_gridcoord(year_coord, "year")
#'
#' # Fit the data for Santa Fe county via an ARIMA model,
#' # not including the last 2 data points:
#' santa_fe <- NM_popcas %>%
#'   dplyr::filter(county == "santafe")
#' sf_coord <- nm_county_coord %>%
#'   dplyr::filter(county == "santafe")
#' n <- seq(1, nrow(santa_fe) - 2)
#' xreg = as.matrix(santa_fe[n, "population", drop = FALSE])
#' fit_Arima <- Arima(santa_fe$count[n],
#'                    order = c(2, 1, 0),
#'                    xreg = xreg[n, , drop = FALSE])
#'
#' # Then use extract_yhat to generate predictions:
#' # Because we did not include the last 2 points in fitting the model,
#' # we set n_ahead = 2
#' extract_yhat(sf_coord, year_coord, fit_Arima, santa_fe,
#'              xreg, n_ahead = 2)
#' }
extract_yhat.forecast_ARIMA <- function(space_coord, time_coord, fit, data, xreg, n_ahead, ...) {
  if (!requireNamespace("forecast", quietly = TRUE)) {
    stop("The 'forecast' package is required to use ARIMA models in statsurv; ",
      "please install it before continuing")
  }
  if (is_multiariate(space_coord, data)) {
    stop("Currently, only uni-ariate ARIMA models can be used in statsurv. ",
         "If using 'loop_model', set 'model_arity = \"uni\"")
  }
  space_label <- names(space_coord)[[1]]
  time_label <- names(time_coord)[[1]]
  if (missing(xreg)) {
   new_predictions <- as.numeric(
     forecast::forecast(fit, h = n_ahead)$mean
   )
  } else {
    rows_to_use <- seq(nrow(xreg) - n_ahead + 1, nrow(xreg))
    new_xreg <- xreg[rows_to_use, , drop = FALSE]
    new_predictions <- as.numeric(forecast::forecast(fit, h = n_ahead, xreg = new_xreg)$mean)
  }
  old_predictions <- as.numeric(fit$fitted)
  predictions <- c(old_predictions, new_predictions)
  yhat <- data.frame(v1 = data[[space_label]],
                     v2 = data[[time_label]],
                     v3 = predictions,
                     stringsAsFactors = FALSE)
  colnames(yhat) <- c(space_label, time_label, "yhat")
  return(yhat)
}

sample_yhat.Arima <- function(space_coord, time_coord, fit, data, n_samples = 10) {
  stop("sample_yhat cannot be used on Arima models - use extract_yhat instead")
}

#' @export
extract_yhat.merMod <- function(space_coord, time_coord, fit, data, ...) {
  if (!requireNamespace("lme4", quietly = TRUE)) {
    stop("The 'lme4' package is required to use lmerMod or glmerMod models in statsurv; ",
         "please install it before continuing")
  }
  sc_name <- gridcoord::gc_get_name(space_coord)
  tc_name <- gridcoord::gc_get_name(time_coord)
  # There is an issue in predict.merMod where the offset is not taken into account on new data
  # when the offset is specified as an option to the glmer function call.
  # https://github.com/lme4/lme4/issues/447
  if ("(offset)" %in% colnames(stats::model.frame(fit))) {
    stop("predict.merMod does not work if an offset is included as an option in glmer.",
         " This is a known bug in the lme4 package.",
         "As a workaround, offsets can be specified as a term in the model formula ",
         "via 'offset(VAR)'")
  }
  yhat <- data.frame(v1 = data[[sc_name]],
                     v2 = data[[tc_name]],
                     v3 = stats::predict(fit, data, type = "response"),
                     stringsAsFactors = FALSE)
  colnames(yhat) <- c(sc_name, tc_name, "yhat")
  return(yhat)
}


predict_from_coeff <- function(coeffs, fit, data) {
  fit$coefficients <- coeffs
  stats::predict(fit, newdata = data, type = "response")
}

get_inla_link  <- function(fit) {
  family <- fit$.args$family
  link_name <- fit$.args$control.family[[1]]$link
  # Now we do some terrible things with INLA and evaluating character strings
  if (link_name == "default") {
    link_name <- INLA::inla.models()$likelihood[[family]]$link[[2]]
  }
  function_name <- paste0("INLA::",
                          "inla.link.",
                          link_name)
  inla_link <- eval(parse(text = function_name))
  return(inla_link)
}

validate_yhat <- function(space_coord, time_coord, yhat) {
  check_type(yhat, "data.frame")
  required_names <- c(gridcoord::gc_get_name(space_coord),
                      gridcoord::gc_get_name(time_coord))
  n_names <- length(required_names)
  yhat_names <- colnames(yhat)
  if (!all(required_names %in% yhat_names[1:n_names])) {
    stop("yhat is not formed correctly. ",
         "The first columns of the yhat data.frame must be the same as ",
         "the names of the space and time coordinates")
  }
  if (ncol(yhat) <= n_names) {
    stop("The yhat data.frame must have one or more columns of predicted data")
  }
  return(yhat)
}


#' @title Average sampled model predictions
#'
#' @description Average model predictions generated by \code{\link{sample_yhat}}.
#'
#' @param space_coord A gridcoord object (\code{\link[gridcoord]{gc_gridcoord}}) describing the
#'   spatial area that is covered by `yhat`.
#' @param time_coord A gridcoord object (\code{\link[gridcoord]{gc_gridcoord}}) describing the time
#'   span that is covered by `yhat`.
#' @param yhat A data.frame created by \code{\link{sample_yhat}} whose first two columns correspond
#'   to `space_coord` and `time_coord`, and whose remaining columns are random samples of the model
#'   predictions.
#'
#' @return A data.frame whose first two columns correspond to `space_coord` and `time_coord`, and
#'   whose third column is the mean of all the samples in `yhat`.
#' @export
#' @md
#' @seealso \code{\link{sample_yhat}}
#' @examples
#' x <- rnorm(20, mean = 0, sd = 5)
#' y <- 1.3 + 2.5 * x + rnorm(20, sd = 1)
#' fit <- lm(y ~ x)
#'
#' new_data <- data.frame(space = c("s1"),
#'                        time = c("t1", "t2", "t3"),
#'                        x = c(-3, 3, 8),
#'                        stringsAsFactors = FALSE)
#' space_coord <- data.frame(space = "s1",
#'                           stringsAsFactors = FALSE)
#' time_coord <- data.frame(time = c("t1", "t2", "t3"),
#'                          stringsAsFactors = FALSE)
#'
#' yhat <- sample_yhat(space_coord,
#'                     time_coord,
#'                     fit,
#'                     new_data,
#'                     n_samples = 10)
#'
#' collapse_yhat(space_coord, time_coord, yhat)
collapse_yhat <- function(space_coord, time_coord, yhat) {
  required_names <- c(gridcoord::gc_get_name(space_coord),
                      gridcoord::gc_get_name(time_coord))
  n_names <- length(required_names)
  yhat_samples <- yhat[, (n_names + 1):ncol(yhat), drop = FALSE]
  collapsed_samples <- rowMeans(yhat_samples)
  collapsed_yhat <- cbind(yhat[, 1:n_names, drop = FALSE], collapsed_samples)
  names(collapsed_yhat)[[3]] <- "yhat"
  return(collapsed_yhat)
}

parse_yhat_extractor <- function(var) {
  # Possible values:
  # NULL - do nothing
  # "extract" - use the extract_yhat
  # "sample" - use the sample
  # A path to a file - use the magic function loader
  # A function - use that function
  if (is.null(var)) {
    return(var)
    }
  if (is.function(var)) {
    return(var)
    }

  if (is.character(var)) {
    if (var == "extract") {
      return(extract_yhat)
    }

    if (var == "sample") {
      return(sample_yhat)
    }

    if (file.exists(var)) {
      return(magic_function_loader(var))
    }

  }
  stop("Unable to understand the yhat_extractor_func argument. ",
       "Allowed character values are NULL, 'extract', 'sample', a custom function, ",
       "or the path to the file containing a custom function")
}

