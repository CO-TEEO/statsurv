# `statsurv`: A package for developing and performing statistical surveillance

Statistical surveillance is any sort of systematic, ongoing analysis aimed at
detecting changes in outcomes as they occur.  If the amount of data reported is large or if
there is significant natural variability in the rate of case events, statistical techniques can
be extremely helpful in determining when a change in the number of events is important, and
when it is merely noise.  The `statsurv` package is designed to help with both designing a
statistical surveillance system and implementing it for routine use. One key part of this is
making it easy to apply statistical surveillance to varying "windows" of data, to mimic of the
process of routinely implementing a surveillance technique over time. This allows the
performance of different statistical surveillance methods to be compared and evaluated.

## Overview:
  
  The `statsurv` package is designed for both performing statistical surveillance and for
  developing and evaluating those statistical surveillance methods. Typically, to conduct
  statistical surveillance, we need to collect and aggregate measurements of the variable of
  interest, and compare those measurements to a baseline estimate, typically generated by a
  model. This process is then repeated as additional measurements are collected over time.

  The functions in the package fall into 5 main categories:
  
  1. Functions for preparing data for analysis
  1. Functions making it easier to generate predictions from models
  1. Functions for comparing outcomes with baseline estimates (scan statistic functions)
  1. Functions used to generate reports or summaries of the surveillance outcomes
  1. Functions for applying all the previous functions of varying data windows


When developing a statistical surveillance system from scratch, generally the first steps will be
to collect and analyze the data, compare it to a model, and run it through a scan statistic to
generate results. The first four categories of functions are all used in this process. Then, the
process must be repeated over varying data windows to see how the system would have performed if
it had been in routine use since data collection began. Only then can the performance of the
system, including the average time to detect a simulated cluster and the average rate of false
alarms, be evaluated.
